---
title: 第十七天 记录 python多进程multiprocessing
categories: 读书笔记
tags: 读书笔记-Python
keywords: [Python]
description: Python基本操作

---

<!--more-->

[TOC]

# 第十七天 记录
## 1. multiprocessing模块

## 2. 多进程爬虫

```python
#!/usr/bin/env python
# encoding: utf-8

"""
@Author: Snaker95
@Software: PyCharm
@File: process_crawler.py
@Time: 2017/3/22 21:25
"""

#!/usr/bin/env python
# encoding: utf-8
import re
import urlparse
import threading
import multiprocessing
import time
from pymongo import MongoClient


from downloader_class import Downlaoder
from mongo_queue import MongoQueue
from zip_csv import read_zip_csv
from loggingprint import lprint

SLEEP_TIME = 1

class ThreadedCrawler(object):
    def __init__(self,seed_url, link_regex, *args, **kwargs):
        # base url
        self.seed_url = seed_url
        # the regex for crawl link
        self.link_regex = link_regex
        # headers
        self.headers = kwargs['headers'] if 'headers' in kwargs else {}
        # to install the proxy
        self.proxy = kwargs['proxy'] if 'proxy' in kwargs else None
        # the count of the downloading site if fail
        self.striven_num = kwargs['striven_num'] if 'striven_num' in kwargs else 2
        # the load speed
        self.delay = kwargs['delay'] if 'delay' in kwargs else 1
        # crawl_depth
        self.crawl_depth = kwargs['crawl_depth'] if 'crawl_depth' in kwargs else 0
        # max threads
        self.threads = kwargs['threads'] if 'threads' in kwargs else 10
        # mongo queue
        uri = "mongodb://root:root123@127.0.0.1:27017"
        self.crawl_queue = MongoQueue(client=MongoClient(uri))


    def crawler(self, call_back=None, cache=None, threads=10):
        # Set threads num
        self.threads = (threads, self.threads)[self.threads == threads]
        # the queue of URL`s that still need to be crawled
        self.crawl_queue.push(self.seed_url)

        D = Downlaoder(cache=cache)


        def process_queue():
            while True:
                try:
                    url = self.crawl_queue.pop()
                except IndexError:
                    # the crawl_queue is empty
                    break
                else:
                    html = D(url)
                    # filter for links matching our regular expression
                    links = self.get_link(html)
                    # zipfile = 'top-1m.csv.zip'
                    # try:
                    #     links = read_zip_csv(zipfile,max=10) or []
                    # except KeyError:
                    #     links = []
                    if call_back:
                        call_back(url, html)

                    for link in links:
                        if re.match(self.link_regex, link):
                            if not re.match(r'^http(s)?://.*', link):
                                # get absolute link
                                link = urlparse.urljoin(self.seed_url, link)
                            self.crawl_queue.push(link)
                    self.crawl_queue.complete(url)
        threads_list = []
        while threads_list or self.crawl_queue:
            # the crawl is still active
            for thread in threads_list:
                if not thread.is_alive():
                    # remove the stopped thread
                    threads_list.remove(thread)

            while len(threads_list) < self.threads and self.crawl_queue:
                # can start some more threading
                thread_handler = threading.Thread(target=process_queue)
                # set daemon so main thread can exit when receives ctrl-c
                thread_handler.setDaemon(True)
                thread_handler.start()
                threads_list.append(thread_handler)

            # all threads have been processed
            # sleep temporarily so CPU can focus execution else where
            time.sleep(SLEEP_TIME)

    # 获取网页中其他深度链接
    @staticmethod
    def get_link(html):
        """Return a list of links from html"""
        # a regular expression to extract all links from the webpage(html)
        webpage_regex = re.compile(r'<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
        # list of all links from the webpage(html)
        return webpage_regex.findall(html)

    # 检查域名是否一致
    @staticmethod
    def same_domain(url1, url2):
        """Return True if both URL's belong to same domain
        """
        return urlparse.urlparse(url1).netloc == urlparse.urlparse(url2).netloc

# multiprocessing
class process_crawler(object):
    def __init__(self, seed_url, link_regex, *args, **kwargs):
        # get the number of CPU
        cpu_count = multiprocessing.cpu_count()

        processes = []
        thread_crawler = ThreadedCrawler(url, link_regex)

        for i in range(cpu_count):
            p = multiprocessing.Process(target=thread_crawler.crawler, args=[args], kwargs=kwargs)
            p.start()
            processes.append(p)

        # wait for processes to complete
        for p in processes:
            p.join()

if __name__ == "__main__":
    start = time.time()
    url = 'http://example.webscraping.com'
    # html = ThreadedCrawler(url, r'/view')
    # html =  html.crawler(threads=1)

    process_crawler(url, r'/view', threads=1)
    end = time.time()
    lprint(end - start)
```

## 3. 使用MongoDB做简单队列

```python
#!/usr/bin/env python
# encoding: utf-8

"""
@Author: Snaker95
@Software: PyCharm
@File: mongo_queue.py
@Time: 2017/3/22 20:44
This is a queue based with MongoDB
"""
from pymongo import MongoClient, errors
from datetime import datetime, timedelta
class MongoQueue(object):
    # possible states of a download
    OUTSTANDING, PROCESSING, COMPETE = range(3)

    def __init__(self, client=None, timeout=300):
        self.client = MongoClient() if client is None else client
        self.db = self.client.cache
        self.timeout = timeout

    # The function is to determine the boolean of the class
    def __nonzero__(self):
        """Return True if there are more jobs to process"""
        record = self.db.crawl_queue.find_one(
            {'status': {'$ne': self.COMPETE}}
        )
        return True if record else False

    def push(self, url):
        """add new url to queue if does not exist"""
        try:
            self.db.crawl_queue.insert(
                {'_id': url, 'status': self.OUTSTANDING}
            )
        except errors.DuplicateKeyError as e:
            pass # this is alreadly in the queue

    def pop(self):
        """
        Get an outstanding URL from the queue and set its status to pocessiong.
        If the queue is empty a KeyError exception is raised
        """
        record = self.db.crawl_queue.find_and_modify(
            query={'status': self.OUTSTANDING},
            update={'$set': {'status': self.PROCESSING, 'timestamp': datetime.now()}}
        )
        if record:
            return record['_id']
        else:
            self.repair()
            raise KeyError()

    def complete(self, url):
        self.db.crawl_queue.update(
            {'_id': url},
            {'$set': {'status': self.COMPETE}}
        )

    def repair(self):
        """Release stalled jobs"""
        record = self.db.crawl_queue.find_and_modify(
            query={
                'timestamp': {'$lt': datetime.now() - timedelta(seconds=self.timeout)},
                'status' : {'$ne': self.COMPETE}
            },
            update={'$set': {'status': self.OUTSTANDING}}
        )
        if record:
            print 'Release: %s'%record['_id']

if __name__ == "__main__":
    pass
```

Author [@Snaker95][1]

[1]: http://www.sharedsea.com