---
title: 第十一天 记录 Python urllib2和第三方模块python-whois
categories: 读书笔记
tags: 读书笔记-Python
keywords: [Python]
description: Python urllib2和第三方模块python-whois

---

<!--more-->

[TOC]

# 第十一天 记录
## 1. python-whois模块
对域名的所有者进行查询

```python
#!/usr/bin/env python
# encoding: utf-8

# pip install python-whois
import whois

def domain_whois(domain):
    try:
        return whois.whois(domain)
    except:
        print 'no domain'
        return None

if __name__ == "__main__":
    print domain_whois('gcjxgj.cn')
```

## 2. urllib2模块
通过urllib2模块读取和下载网页信息

```python
#!/usr/bin/env python
# encoding: utf-8

"""
The class is a class for download site page
通过使用urllib2 下载并读取网页
"""
import random
import urllib2
import urlparse
import socket
import time, datetime
import re

from loggingprint import lprint

class Downlaoder(object):
    def __init__(self,headers=None, proxies=None, delay=5, striven_num=2,
                 timeout=30, cache=None, data=None, *args, **kwargs):
        socket.setdefaulttimeout(timeout)
        self.throttle = Throttle(delay)
        self.headers = {
            "User-Agent": "Mozilla/5.0"
            ,"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
            ,"Accept-Language": "en-us,en;q=0.5"
            ,"Accept-Encoding": "gzip,deflate"
            ,"Keep-Alive": "300"
            ,"Connection": "keep-alive"
        }
        if headers is not None:
            self.headers.update(headers)
        self.proxies = proxies
        self.striven_num = striven_num
        self.cache = cache
        self.data = data

    def __call__(self, url):
        result = None
        if self.cache:
            try:
                result = self.cache[url]
            except KeyError as e:
                result = None
            else:
                if self.striven_num > 0 and 500 <= result['code'] < 600:
                    result = None
        if result is None:
            self.throttle.wait(url)
            proxy = random.choice(self.proxies) if self.proxies else None
            headers = self.headers
            result = self.download(url,headers,proxy,self.striven_num,self.data)
            if self.cache:
                self.cache[url] = result
        return result['html']

    def download(self, url, headers, proxy=None, striven_num=2, data=None):
        if not re.match(r'^http(s)?://.*',url):
            url = 'http://%s'%url

        request = urllib2.Request(url, data, headers or {})
        # 设置代理
        opener = urllib2.build_opener()
        if proxy:
            proxy_params = {urlparse.urlparse(url).scheme: proxy}
            opener.add_handler(urllib2.ProxyHandler(proxy_params))
        try:
            response = opener.open(request)
            html = response.read()
            code = response.code
        except urllib2.URLError as e:
            lprint('Download Error : %s' % (e.reason))
            html = ''
            code = 404
            # when 5XX then try again
            if self.striven_num > 1 and hasattr(e, 'code') and 500 <= e.code < 600:
                return self.download(url, headers, proxy, striven_num - 1, data)

        return {'html':html, 'code':code}



class Throttle(object):
    """Add a delay between downloads to the same domain"""
    def __init__(self, delay):
        # amount of delay between downloads for each domain
        self.delay = delay
        # timestamp of when a domain was last accessed
        self.domains = {}

    def wait(self, url):
        domain = urlparse.urlparse(url).netloc
        # {domain:time_object}
        last_accessed = self.domains.get(domain)

        if self.delay > 0 and last_accessed is not None:
            sleep_secs = self.delay - (datetime.datetime.now()-last_accessed).seconds
            if sleep_secs > 0 :
                # domain has been accessed recently
                # so need to sleep
                time.sleep(sleep_secs)
        # update the last accessed timeDiskCache
        self.domains[domain] = datetime.datetime.now()

if __name__ == "__main__":
    url = 'http://wx.gcjxgj.cn/DbDeviceLease/ajaxList.html'
    # html = Downlaoder()(url)
    # downloader = Downlaoder()
    # html = downloader(url)
    try:
        from diskcache_class import DiskCache
        from mongocache_class import MongoCache
        from datetime import timedelta
        # cache = DiskCache(expires=timedelta(seconds=10000))
        cache = MongoCache(expires=timedelta(seconds=10))
        headers = {
            'User-agent':'wswp', 
            'Content-Type':'application/x-www-form-urlencoded', 
            'X-Requested-With':'XMLHttpRequest'
        }
        downloader = Downlaoder(headers=headers,cache=cache)
        html = downloader(url)

        # cache[url] = html
        # del(cache[url])
        # lprint(cache[url])
        # cache.clean()
    except Exception as e:
        print e
    lprint(html)
```

## 3. 第三方模块itertools

```python
# encoding: utf-8
# module itertools
# from lib/python2.7/lib-dynload/itertools.so
# by generator 1.144
"""
Functional tools for creating and using iterators.

Infinite iterators:
count([n])                  --> n, n+1, n+2, ...
cycle(p)                    --> p0, p1, ... plast, p0, p1, ...
repeat(elem [,n])           --> elem, elem, elem, ... endlessly or up to n times

Iterators terminating on the shortest input sequence:
chain(p, q, ...)            --> p0, p1, ... plast, q0, q1, ... 
compress(data, selectors)   --> (d[0] if s[0]), (d[1] if s[1]), ...
dropwhile(pred, seq)        --> seq[n], seq[n+1], starting when pred fails
groupby(iterable[, keyfunc])--> sub-iterators grouped by value of keyfunc(v)
ifilter(pred, seq)          --> elements of seq where pred(elem) is True
ifilterfalse(pred, seq)     --> elements of seq where pred(elem) is False
islice(seq, [start,] stop [, step]) 
                            --> elements from seq[start:stop:step]
imap(fun, p, q, ...)        --> fun(p0, q0), fun(p1, q1), ...
starmap(fun, seq)           --> fun(*seq[0]), fun(*seq[1]), ...
tee(it, n=2)                --> (it1, it2 , ... itn) splits one iterator into n
takewhile(pred, seq)        --> seq[0], seq[1], until pred fails
izip(p, q, ...)             --> (p[0], q[0]), (p[1], q[1]), ... 
izip_longest(p, q, ...)     --> (p[0], q[0]), (p[1], q[1]), ... 

Combinatoric generators:
product(p, q, ... [repeat=1]) 
                            --> cartesian product
permutations(p[, r])
combinations(p, r)
combinations_with_replacement(p, r)
"""
```

Author [@Snaker95][1]

[1]: http://www.sharedsea.com