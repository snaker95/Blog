---
title: 第十三天 记录 Python文件操作-磁盘缓存网页
categories: 读书笔记
tags: 读书笔记-Python
keywords: [Python]
description: Python文件操作-磁盘缓存网页

---

<!--more-->

[TOC]

# 第十三天 记录
## 1. 磁盘缓存网页
在网页抓取数据过程中,为了减少同一网页的重复性请求网络, 缓存网页信息就非常必要, 下面是根据书本内容完成的;

```python
#!/usr/bin/env python
# encoding: utf-8

"""
@Author: Snaker95
@Software: PyCharm
@File: diskcache_class.py
@Time: 2017/3/13 09:18

The class is a function to download site page in the disk
the filename that the cache of site page is base of url
"""
import re
import os
import urlparse
import cPickle
import zlib
from datetime import datetime, timedelta
import shutil

from loggingprint import lprint

class DiskCache(object):
    def __init__(self, cache_dir='cache',expires=timedelta(days=1)):
        self.cache_dir = cache_dir
        self.compress = False
        self.expires = expires
	 # 是否压缩
    def set_compress(self,compress=True):
        self.compress = compress
        return self
	# 设置过期时间
    def set_expires(self, expires=timedelta(days=1)):
        self.expires = expires
        return self

    # 将url转化成相应的目录文件
    def url_to_path(self, url):
        """Create file system path for this URL"""

        components = urlparse.urlsplit(url)
        path = components.path
        # append index.html if paths is empty
        if not path:
            path = '/index.html'
        elif path.endswith('/'):
            path = ''.join((path, 'index.html'))
        query_pre = ''
        if components.query:
            query_pre = '_'
        filename = '%s%s%s%s'%(components.netloc, path, query_pre, components.query)
        # replace invalid characters
        filename = re.sub(r'[^0-9a-zA-z_./]', '_', filename)
        # restrict maxinum number of characters
        filename = '/'.join(segment[:255] for segment in filename.split('/'))
        return os.path.join(self.cache_dir, filename)

	 # 采用对象方式获取key对应的value
    def __getitem__(self, url):
        """Load data from disk for this URL"""
        path = self.url_to_path(url)
        if os.path.exists(path):
            # if self.compress is True:
            #     with open(path,'rb') as fp:
            #         return cPickle.loads(zlib.decompress(fp.read()))
            # else:
            #     with open(path,'rb') as fp:
            #         return cPickle.load(fp)
            with open(path, 'rb') as fp:
                data = fp.read()
                if self.compress is True:
                    data = zlib.decompress(data)

                result, timestamp = cPickle.loads(data)

                if self.has_expired(timestamp):
                    raise KeyError('%s is expired'%url)
                # return cPickle.load(fp)

            return result
        else:
            # URL has not yet been cached
            raise KeyError('%s does not exist'%url)

	 # 设置key-value键值对
    def __setitem__(self, url, value):
        """Save data to disk for this URL"""
        path = self.url_to_path(url)
        folder = os.path.dirname(path)
        if not os.path.exists(folder):
            os.makedirs(folder)
        # 使用cPickle处理文本信息
        data = cPickle.dumps((value, datetime.utcnow()))
        # if self.compress is True:
        #     with open(path, 'wb') as fp:
        #         fp.write(zlib.compress(cPickle.dumps(value)))
        # else:
        #     with open(path, 'wb') as fp:
        #         fp.write(cPickle.dumps(value))
        with open(path, 'wb') as fp:
            if self.compress is True:
                data = zlib.compress(data)
            fp.write(data)
	 
	 # 删除key-value, 对应文件也删除
    def __delitem__(self, url):
        """Remove the value at this key and any empty parent sub-directories
        """
        path = self.url_to_path(url)
        try:
            os.remove(path)
            os.removedirs(os.path.dirname(path))
        except OSError:
            pass

    # 判断缓存文件是否过期
    def has_expired(self, timestamp):
        """Return whether this timestamp has expired"""
        return datetime.utcnow() > timestamp + self.expires

	 # 清除缓存目录
    def clean(self):
        """Remove all the cache values"""
        try:
            if os.path.exists(self.cache_dir):
                    shutil.rmtree(self.cache_dir)
        except OSError as e:
            pass
	 # 清除某域的缓存
    def clean_domain(self,domain=''):
        domain = os.path.join(self.cache_dir, urlparse.urlparse(domain).netloc)
        try:
            if os.path.exists(domain):
                    shutil.rmtree(domain)
        except OSError as e:
            pass

if __name__ == "__main__":
    url = 'https://wx.gcjxgj.cn/dbmember/id/1.html?search=123#3'
    cache = DiskCache(expires=timedelta(seconds=10)).set_compress(False)
    # lprint(cache.url_to_path(url))
    # cache[url] = ''
    lprint(cache[url])
```

## 2. zlib压缩模块

```python
"""
The functions in this module allow compression and decompression using the
zlib library, which is based on GNU zip.

adler32(string[, start])                -- Compute an Adler-32 checksum.
compress(string[, level])               -- Compress string, with compression level in 0-9.
compressobj([level])                    -- Return a compressor object.
crc32(string[, start])                  -- Compute a CRC-32 checksum.
decompress(string,[wbits],[bufsize])    -- Decompresses a compressed string.
decompressobj([wbits])                  -- Return a decompressor object.

'wbits' is window buffer size.
Compressor objects support compress() and flush() methods; decompressor
objects support decompress() and flush().
"""
```

**备注:**
*该模块仅支持压缩和解压缩, 若将压缩或者解压缩写入到文件,还需配合文件操作的其他模块*
Author [@Snaker95][1]

[1]: http://www.sharedsea.com