---
title: 第十六天 记录 python多线程threading.Thread
categories: 读书笔记
tags: 读书笔记-Python
keywords: [Python]
description: python多线程

---

<!--more-->

[TOC]

# 第十六天 记录
## 1. threading.Thread模块
* 这个类表示在一个可以单独控制线程运行的活动。有两种指定这个活动的方法：1.通过传递一个可调用对象给构造函数，2.在子类中覆盖run()方法。在子类中不应该覆盖其它方法（构造函数除外）;换句话说，就是只覆盖该类的__init__()和run()方法。

```python
#!/usr/bin/env python
# encoding: utf-8
import threading

# 1. 传递可调用对象(方法)给构造函数
def test_func():
    # 操作
    pass
thread_handler = threading.Thread(target=test_func, args=(), kwargs={})
## test_func : 为函数或者class
## args : 为使用test_func时传入的参数
## kwargs: 为使用test_func时传入的字典参数  

# 2. 使用子类覆盖
class MyThread(threading.Thread):
    def __init__(self, **args, **kwargs):
        # 构造函数, 初始化
        pass
    
    def run(self):
        # 操作, 类似1.中test_func
        pass
mythread_handler = MyThread(args=(), kwargs={})
```

* 一个线程对象创建后，它的活动必须通过调用线程的start()方法启动。它在一个单独的控制线程中调用run()方法。

```python
# 1. 
thread_handler.start()
## 启动线程, 执行threading.Thread的run(), 此时run()会调用func
'''
# threading.Thread中的runfangfa
def run(){
	try:
	    if self.__target:
	        self.__target(*self.__args, **self.__kwargs)
	finally:
	    # Avoid a refcycle if the thread is running a function with
	    # an argument that has a member that points to the thread.
	    del self.__target, self.__args, self.__kwargs
}
'''
	
# 2. 
mythread_handler.start()
## 启动线程, 执行MyThread类中的run()
```

* 相关类内函数
	1. **class threading.Thread(group=None, target=None, name=None, args=(), kwargs={})**
	> * `group`应该为None；被保留用于未来实现了ThreadGroup类时的扩展。
	> * `target`是将被run()方法调用的可调用对象。默认为None，表示不调用任何东西。
	> * `name`是线程的名字。默认情况下，以“Thread-N”的形式构造一个唯一的名字，N是一个小的十进制整数。
	> * `args`是给调用目标的参数元组。默认为()。
	> * `kwargs`是给调用目标的关键字参数的一个字典。默认为{}。
	> * <small>如果子类覆盖该构造函数，它必须保证在对线程做任何事之前调用基类的构造函数(Thread.__init__())。</small>
	
	2.  **start()**
	> * 开始线程活动
	> * 在同一线程对象上多次调用该方法将引发一个RuntimeError。
	
	3. **run()**
	> * 表示线程活动的方法。
	
	4. **join([timeout])**
	> * 等待直至线程终止。它阻塞调用线程直至该线程终止 – (1)正常终止;(2)通过一个未处理的异常或者直至timeout发生。
	> * 当`timeout`参数存在且不为None时，它应该以一个浮点数指定该操作的超时时间，单位为秒（可以是小数）。由于join()永远返回None，你必须在join()之后调用isAlive()来判断超时是否发生 – 如果线程仍然活着，则join()调用超时。
	> * 如果timeout参数不存在或者为None，那么该操作将阻塞直至线程终止
	> * 一个线程可以被join()多次。
	> * 如果尝试join当前的线程，join()会引发一个RuntimeError，因为这将导致一个死锁。在线程启动之前join()它也是错误的，尝试这样做会引发同样的异常。
	
	5. **name**
	> * 一个字符串，只用于标识的目的。它没有语义。多个线程可以被赋予相同的名字。初始的名字通过构造函数设置。
	
	6. **getName()/ setName()**
	> * 获取/设置线程的名字, 最好在启动线程之前
	
	7. **ident** 
	> * 线程的ID，如果线程还未启动则为None。它是一个非零的整数。参见`thread.get_ident()`函数。当一个线程退出另外一个线程创建时，线程的ID可以重用。即使在线程退出后，其ID仍然可以访问。
	
	8. **is_alive()/ isAlive()**
	> * 返回线程是否还活着。
	> * 在run()方法刚开始之前至run()方法刚终止之后，该方法返回True。threading.enumerate()返回所有活着的函数的一个列表。
	
	9. **daemon**
	> * 一个布尔值，指示线程是(True)否(False)是一个守护线程。它必须在调用start()之前设置，否则会引发RuntimeError。它的初始值继承自创建它的线程；主线程不是一个守护线程，所以在主线程中创建的所有线程默认daemon = False。
	
	10. **isDaemon()/ setDaemon()**
	> * 检测是否为守护进程/ 设置守护进程

## 2. threading.Lock和threading.RLock

```python
"""
* Lock.acquire([blocking]) / RLock.acquire([blocking=1])
	获取一把锁，阻塞的或者非阻塞的。
	当调用时blocking参数设置为True（默认值），将阻塞直至锁变成unblocked，然后设置它的状态为locked并返回True。
	当调用时blocking参数设置为False，将不会阻塞。If a call with blocking set to True would block, return False immediately; otherwise, set the lock to locked and return True.
	
* Lock.release() / RLock.release()
	释放一把锁。
	当锁是locked时，重置它为unlocked，然后返回。如果存在其他阻塞的线程正在等待锁变成unblocked状态，只会允许它们中的一个继续。
	在一把没有锁住的锁上调用时，引发一个ThreadError 。
	没有返回值。
"""
```

## 3. threading模块实践

```python
#!/usr/bin/env python
# encoding: utf-8
import re
import urlparse
import threading
import time

from downloader_class import Downlaoder
from zip_csv import read_zip_csv

SLEEP_TIME = 1

class Threaded_crawler(object):
    def __init__(self,seed_url, link_regex, *args, **kwargs):
        # base url
        self.seed_url = seed_url
        # the regex for crawl link
        self.link_regex = link_regex
        # headers
        self.headers = kwargs['headers'] if 'headers' in kwargs else {}
        # to install the proxy
        self.proxy = kwargs['proxy'] if 'proxy' in kwargs else None
        # the count of the downloading site if fail
        self.striven_num = kwargs['striven_num'] if 'striven_num' in kwargs else 2
        # the load speed
        self.delay = kwargs['delay'] if 'delay' in kwargs else 1
        # crawl_depth
        self.crawl_depth = kwargs['crawl_depth'] if 'crawl_depth' in kwargs else 0
        # max threads
        self.threads = kwargs['threads'] if 'threads' in kwargs else 10

    def crawler(self, call_back=None, cache=None, threads=10):
        # Set threads num
        self.threads = (threads, self.threads)[self.threads == threads]
        # the queue of URL`s that still need to be crawled
        crawl_queue = [self.seed_url]

        # the URL`s that have been seen
        seen = set(crawl_queue)

        D = Downlaoder(cache=cache)


        def process_queue():
            print thread_handler.getName()
            while True:
                try:
                    url = crawl_queue.pop()
                except IndexError:
                    # the crawl_queue is empty
                    break
                else:
                    html = D(url)
                    # filter for links matching our regular expression
                    links = self.get_link(html)
                    if call_back:
                        call_back(url, html)

                    for link in links:
                        if re.match(self.link_regex, link):
                            if not re.match(r'^http(s)?://.*', link):
                                # get absolute link
                                link = urlparse.urljoin(self.seed_url, link)
                            # check if seed have already this link
                            if self.same_domain(self.seed_url, link) and link not in seen:
                                crawl_queue.append(link)
                                seen.add(link)
        threads_list = []
        while threads_list or crawl_queue:
            # the crawl is still active
            for thread in threads_list:
                if not thread.is_alive():
                    # remove the stopped thread
                    threads_list.remove(thread)

            while len(threads_list) < self.threads and crawl_queue:
                # can start some more threading
                thread_handler = threading.Thread(target=process_queue)
                # set daemon so main thread can exit when receives ctrl-c
                thread_handler.setDaemon(True)
                thread_handler.start()
                threads_list.append(thread_handler)

            # all threads have been processed
            # sleep temporarily so CPU can focus execution else where
            time.sleep(SLEEP_TIME)

    # 获取网页中其他深度链接
    @staticmethod
    def get_link(html):
        """Return a list of links from html"""
        # a regular expression to extract all links from the webpage(html)
        webpage_regex = re.compile(r'<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
        # list of all links from the webpage(html)
        return webpage_regex.findall(html)

    # 检查域名是否一致
    @staticmethod
    def same_domain(url1, url2):
        """Return True if both URL's belong to same domain
        """
        return urlparse.urlparse(url1).netloc == urlparse.urlparse(url2).netloc


if __name__ == "__main__":
    start = time.time()
    url = 'http://example.webscraping.com'
    html = Threaded_crawler(url, r'^http://')
    html =  html.crawler(threads=8)
    end = time.time()

    print end - start

```

Author [@Snaker95][1]

[1]: http://www.sharedsea.com